# -*- coding: utf-8 -*-
"""Assigment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MJ8DCH7-Y6-EsCAT074Fcuf3Y9CZDW_Z

## About Dataset

Dataset Name: **BBC Articles Sentiment Analysis Dataset**

Source: BBC News (https://www.kaggle.com/datasets/amunsentom/article-dataset-2)

Description:
This dataset consists of articles from the BBC News website, containing a diverse range of topics such as business, politics, entertainment, technology, sports, and more. The dataset includes articles from various time periods and categories.

Number of Instances: 2126

Features:
- category: The category or topic of the article (e.g., business, politics, sports).
- text: The content of the article (string).
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("amunsentom/article-dataset-2")

print("Path to dataset files:", path)



import pandas as pd
import os

# Busca un archivo CSV en la carpeta descargada
csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]

if csv_files:
    dataset_path = os.path.join(path, csv_files[0])  # Usa el primer CSV encontrado
    df = pd.read_csv(dataset_path)
    print(df.head())  # Muestra las primeras filas del dataset
else:
    print("No se encontraron archivos CSV en la carpeta descargada.")

df.head()

"""# LSTM"""

import numpy as np
import pandas as pd
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

import tensorflow as tf
print("GPU Available:", tf.config.list_physical_devices('GPU'))

# Extract necessary columns
X = df['text'].astype(str).values  # Article text

positive_words = [ 'good','excellent', 'positive', 'fortunate', 'benefit', 'win']
negative_words = ['bad', 'terrible', 'poor', 'negative', 'unfortunate', 'loss', 'fail', 'awful', 'horrible', 'disappointing', 'worse', 'unbearable', 'sick']

def simple_sentiment_label(text):
    text_lower = text.lower()
    pos_count = sum(text_lower.count(word) for word in positive_words)
    neg_count = sum(text_lower.count(word) for word in negative_words)
    #print(neg_count)
    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    else:
        return 'neutral'

df['sentiment'] = df['text'].apply(simple_sentiment_label)

# Cuenta las ocurrencias de cada clase
sentiment_counts = df['sentiment'].value_counts()
print(sentiment_counts)

# Determinar el número mínimo de instancias entre las clases
min_count = sentiment_counts.min()
print(f"El número mínimo de instancias es: {min_count}")

# Filtra las clases para equilibrar
df_positive = df[df['sentiment'] == 'positive'].sample(min_count, random_state=42)
df_negative = df[df['sentiment'] == 'negative']
df_neutral = df[df['sentiment'] == 'neutral']

# Combina las clases equilibradas
df_balanced = pd.concat([df_positive, df_negative, df_neutral])

# Barajar las filas para que el DataFrame quede mezclado
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Verifica los primeros registros del DataFrame equilibrado
print(df_balanced.head())

# Actualizar las etiquetas (Y) con el DataFrame equilibrado
Y_balanced = df_balanced['sentiment'].values
sentiment_map = {'positive': 0, 'neutral': 1, 'negative': 2}
Y_balanced = np.array([sentiment_map[label] for label in Y_balanced])

# Verifica el conteo de las instancias después de equilibrar
print(df_balanced['sentiment'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

# Plot sentiment distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='sentiment', data=df, palette='Set2')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Articles')
plt.show()

# Contar las ocurrencias de cada etiqueta de sentimiento
sentiment_counts = df['sentiment'].value_counts()

# Mostrar el resultado
print("\nCantidad de etiquetas de sentimiento asignadas:")
print(sentiment_counts)

Y = df['sentiment'].values  # Sentiment label

# Convert text labels to numeric values
sentiment_map = {'positive': 0, 'neutral': 1, 'negative': 2}
Y = np.array([sentiment_map[label] for label in Y])

# Tokenize text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
word_index = tokenizer.word_index
X_seq = tokenizer.texts_to_sequences(X)

# Apply padding so that all sequences have the same length
maxlen = max(len(seq) for seq in X_seq)
X_pad = pad_sequences(X_seq, maxlen=maxlen, padding='post', truncating='post')

# Convert labels to one-hot encoding
Y_categorical = to_categorical(Y, num_classes=3)

embedding_dim = 100
glove_path = 'glove.6B.100d.txt'  # Path to GloVe file
embeddings_index = {}

with open(glove_path, 'r', encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Create the embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

from sklearn.manifold import TSNE
import numpy as np

words = list(embeddings_index.keys())[:100]  # Pick first 100 words
word_vectors = np.array([embeddings_index[w] for w in words])

tsne = TSNE(n_components=2, random_state=42)
word_vec_2d = tsne.fit_transform(word_vectors)

plt.figure(figsize=(7, 7))
plt.scatter(word_vec_2d[:, 0], word_vec_2d[:, 1], marker='.')
for i, word in enumerate(words):
    plt.annotate(word, (word_vec_2d[i, 0], word_vec_2d[i, 1]), fontsize=9)
plt.title('Word Embeddings Visualization (t-SNE)')
plt.show()

# Define the LSTM model
model = Sequential([
    Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),
    LSTM(units=12, return_sequences=True),
    LSTM(units=4),
    Dense(3, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

class_weights = {0: 1, 1: 2, 2: 4}  # Ajusta los pesos según el desequilibrio
history = model.fit(X_pad, Y_categorical, epochs=100, batch_size=32, validation_split=0.2, class_weight=class_weights)





# Plot Loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')

plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix

y_pred = model.predict(X_pad)
y_pred_classes = y_pred.argmax(axis=1)
y_true = Y_categorical.argmax(axis=1)

cm = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import precision_recall_curve, roc_curve, auc

y_probs = model.predict(X_pad)

# Precision-Recall Curve
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
precision, recall, _ = precision_recall_curve(Y_categorical.ravel(), y_probs.ravel())
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')

# ROC Curve
plt.subplot(1, 2, 2)
fpr, tpr, _ = roc_curve(Y_categorical.ravel(), y_probs.ravel())
plt.plot(fpr, tpr, marker='.')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')

plt.show()

# Function to predict sentiment of an article
def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    pad_seq = pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')
    prediction = model.predict(pad_seq)
    sentiment = np.argmax(prediction)
    reverse_map = {0: 'positive', 1: 'neutral', 2: 'negative'}
    return reverse_map[sentiment]

# Example prediction for multiple news texts
# Example prediction for multiple news texts (including neutral and negative)
test_texts = [
    # Positive
    "The company reported a significant increase in profits this quarter.",
    "Local communities celebrate after a successful campaign to protect a vital nature reserve from development.",
    "The government has announced new measures to combat the rising cost of living, including tax cuts for low-income families.",

    # Neutral
    "The company has announced its quarterly earnings, which showed steady growth despite market fluctuations.",
    "A new infrastructure project is set to begin next month, with expected completion in two years.",
    "The government is reviewing its policies on immigration in an effort to improve national security.",
    "Researchers are investigating the potential impact of a new drug on human health, with initial tests showing mixed results.",
    "The city council is planning to hold a public consultation regarding proposed changes to local zoning laws.",

    # Negative
    "The local hospital is facing severe budget cuts, leading to staff layoffs and reduced services for patients.",
    "A cyberattack has compromised sensitive personal information of thousands of users across the country.",
    "Protests erupted across the city following the controversial decision to raise fuel prices by 20%.",
    "The company’s recent product launch has faced significant backlash, with customers expressing dissatisfaction over quality issues.",
    "The economic slowdown is causing widespread job losses, with many small businesses struggling to stay afloat."
]

# Loop through each test text and predict sentiment
for i, text in enumerate(test_texts):
    sentiment = predict_sentiment(text)
    print(f"Test {i+1}: {text}\nPredicted Sentiment: {sentiment}\n")


# Loop through each test text and predict sentiment
for i, text in enumerate(test_texts):
    sentiment = predict_sentiment(text)
    print(f"Test {i+1}: {text}\nPredicted Sentiment: {sentiment}\n")

"""# Predictor"""